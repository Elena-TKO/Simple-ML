{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "id": "yk7fQ3LZ-KvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urpYU5vu9nKt"
      },
      "outputs": [],
      "source": [
        "!unzip -q /content/gdrive/MyDrive/Яндекс/train.zip -d train\n",
        "!unzip -q /content/gdrive/MyDrive/Яндекс/test.zip -d test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')\n",
        "    DEVICE = torch.device(\"cuda\")\n",
        "Data_modes = ['train', 'test']"
      ],
      "metadata": {
        "id": "_mypyO8ldvnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "from torch.optim import Adam"
      ],
      "metadata": {
        "id": "S1Dq7kSmf9JS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "#for loading and visualizing audio files\n",
        "import librosa\n",
        "import librosa.display\n",
        "\n",
        "\n",
        "#to play audio\n",
        "import IPython.display as ipd"
      ],
      "metadata": {
        "id": "YwmG3oYVSRee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_fpath = \"/content/train/train/\"\n",
        "audio_clips = os.listdir(audio_fpath)\n",
        "for i,el in enumerate(audio_clips):\n",
        "  if el.split('.')[1] != 'wav':\n",
        "    targets_name = el\n",
        "    i_targets = i\n",
        "audio_clips.pop(i_targets)\n",
        "print(\"No. of .wav files in audio folder = \",len(audio_clips))"
      ],
      "metadata": {
        "id": "mV9At9VaYd1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "targets = pd.read_csv(audio_fpath+targets_name, sep = '\\t', header = None, names = ['filename', 'idx_sex'])\n",
        "targets"
      ],
      "metadata": {
        "id": "eHTRooHKw-mL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1. Get the file path to an included audio example\n",
        "filename = '/content/train/train/0006238dc99eaf68957dfc81826d1071.wav'\n",
        "\n",
        "# 2. Load the audio as a waveform `y`\n",
        "#    Store the sampling rate as `sr`\n",
        "y, sr = librosa.load(filename)\n",
        "\n",
        "# 3. Run the default beat tracker\n",
        "tempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr)\n",
        "\n",
        "print('Estimated tempo: {:.2f} beats per minute'.format(tempo))\n",
        "\n",
        "# 4. Convert the frame indices of beat events into timestamps\n",
        "beat_times = librosa.frames_to_time(beat_frames, sr=sr)"
      ],
      "metadata": {
        "id": "MqTZSgukSLx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(14, 5))\n",
        "librosa.display.waveplot(y, sr=sr)"
      ],
      "metadata": {
        "id": "iBSAOIujYIMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(targets[targets['filename'] == os.path.basename(filename).split('.')[0]])\n",
        "idx_sex_dict = {1:'Female', 2: 'Male'}\n",
        "ipd.Audio(y, rate = sr)"
      ],
      "metadata": {
        "id": "Mgxd2xVqy7QU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128,\n",
        "                                    fmax=8000)"
      ],
      "metadata": {
        "id": "5v3iA1pKcZNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "S_dB = librosa.power_to_db(S, ref=np.max)\n",
        "img = librosa.display.specshow(S_dB, x_axis='time',\n",
        "                         y_axis='mel', sr=sr,\n",
        "                         fmax=8000, ax=ax)\n",
        "fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
        "ax.set(title='Mel-frequency spectrogram')"
      ],
      "metadata": {
        "id": "VbQoO0BYceNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сделаем DataSet спектрограмм"
      ],
      "metadata": {
        "id": "Fk79taWhgP7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioDataset():\n",
        "  def __init__(self,  train_mode =True):\n",
        "    if train_mode:\n",
        "      self.trainset = self.create_dataset('train')\n",
        "      self.validset = self.create_dataset('valid')\n",
        "    self.testset = self.create_dataset('test')\n",
        "    \n",
        "  def return_sex(self, id):\n",
        "      return self.id_sex_dict[id]\n",
        "\n",
        "  def return_data(self):\n",
        "      return self.trainset, self.validset, self.testset\n",
        "\n",
        "  def return_test(self):\n",
        "      return self.testset\n",
        "\n",
        "  def create_dataset(self, mode):\n",
        "      \n",
        "      if mode == 'train' or mode == 'valid':\n",
        "        audio_fpath = \"/content/train/train/\"\n",
        "        audio_clips = os.listdir(audio_fpath)\n",
        "        for i,el in enumerate(audio_clips):\n",
        "          if el.split('.')[1] != 'wav':\n",
        "            targets_name = el\n",
        "            i_targets = i\n",
        "        audio_clips.pop(i_targets)\n",
        "        targets = pd.read_csv(audio_fpath+targets_name, sep = '\\t', header = None, names = ['filename', 'idx_sex'])\n",
        "        spectrograms_and_targets = []\n",
        "        \n",
        "        if mode == 'train':\n",
        "          for filename in tqdm(audio_clips[:10]):\n",
        "            trgt = targets[targets['filename'] == os.path.basename(filename).split('.')[0]]\n",
        "            spectrograms_and_targets.append(\n",
        "                self.preprocess_sample(mode,audio_fpath+filename, trgt))\n",
        "\n",
        "          x,y = map(np.stack, zip(*spectrograms_and_targets))\n",
        "          return x,y\n",
        "\n",
        "        if mode == 'valid':\n",
        "          for filename in tqdm(audio_clips[10:15]):\n",
        "            trgt = targets[targets['filename'] == os.path.basename(filename).split('.')[0]]\n",
        "            spectrograms_and_targets.append(\n",
        "                self.preprocess_sample(mode,audio_fpath+filename, trgt))\n",
        "\n",
        "          x,y = map(np.stack, zip(*spectrograms_and_targets))\n",
        "          return x,y\n",
        "\n",
        "      if mode == 'test':\n",
        "        audio_fpath = \"/content/test/test/\"\n",
        "        audio_clips = os.listdir(audio_fpath)\n",
        "        specs_test = []\n",
        "        for filename in tqdm(audio_clips[:7]):\n",
        "          specs_test.append(\n",
        "              self.preprocess_sample_test(mode,audio_fpath+filename)\n",
        "          )\n",
        "        return specs_test\n",
        "\n",
        "  @staticmethod\n",
        "  def spec_to_image(spec, eps=1e-6):\n",
        "        mean = spec.mean()\n",
        "        std = spec.std()\n",
        "        spec_norm = (spec - mean) / (std + eps)\n",
        "        spec_min, spec_max = spec_norm.min(), spec_norm.max()\n",
        "        spec_scaled = 255 * (spec_norm - spec_min) / (spec_max - spec_min)\n",
        "        spec_scaled = spec_scaled.astype(np.uint8)\n",
        "        return spec_scaled\n",
        "\n",
        "  def preprocess_sample(self, mode ,filename, target = 0, max_length = 100):\n",
        "      amp, sr = librosa.load(filename)\n",
        "      spectrogram = librosa.feature.melspectrogram(amp, sr=sr, n_mels=128, fmin=1, fmax=8000)[:, :max_length]\n",
        "      spectrogram = np.pad(spectrogram, [[0, 0], [0, max(0, max_length - spectrogram.shape[1])]], mode='constant')\n",
        "      return self.spec_to_image(np.float32(spectrogram)), target\n",
        "\n",
        "  def preprocess_sample_test(self, mode ,filename, target = 0, max_length = 100):\n",
        "      amp, sr = librosa.load(filename)\n",
        "      spectrogram = librosa.feature.melspectrogram(amp, sr=sr, n_mels=128, fmin=1, fmax=8000)[:, :max_length]\n",
        "      spectrogram = np.pad(spectrogram, [[0, 0], [0, max(0, max_length - spectrogram.shape[1])]], mode='constant')\n",
        "      \n",
        "      return self.spec_to_image(np.float32(spectrogram))"
      ],
      "metadata": {
        "id": "VehaCWbKgPud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoader():\n",
        "  def __init__(self, spectrograms, targets):\n",
        "    dataset = AudioDataset().return_train()\n",
        "    self.data = list(zip(dataset))\n",
        "\n",
        "  def nex_batch(self, b_size, device):\n",
        "    ind = np.random.randint(len(self.data), size = b_size)\n",
        "\n",
        "    input = [self.data[i] for i in ind]\n",
        "\n",
        "    source = [line[0] for line in input]\n",
        "    target = [line[1] for line in input]\n",
        "\n",
        "    return self.torch_batch(source, target, device)\n",
        "  \n",
        "  @staticmethod\n",
        "  def torch_batch(source, target, device):\n",
        "    return tuple(\n",
        "        [\n",
        "            torch.tensor(val,dtype = torch.float).to(device, non_blocking = True)\n",
        "            for val in [source, target]\n",
        "        ]\n",
        "    )"
      ],
      "metadata": {
        "id": "rk8pVPvvMIUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, valid_data, test_data = AudioDataset().return_data()"
      ],
      "metadata": {
        "id": "bSAkdMWaPMes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = DataLoader(*train_data)\n",
        "testset = DataLoader(*test_data)\n",
        "batch_size = 64\n"
      ],
      "metadata": {
        "id": "NpA4qreCb_gC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выберем модель "
      ],
      "metadata": {
        "id": "nJIL5kB3fu2m"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MpKj0pi0WmB"
      },
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, window_sizes=(3, 4, 5)):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv2d(1, 128, [window_size, 128], padding=(window_size - 1, 0))\n",
        "            for window_size in window_sizes\n",
        "        ])\n",
        "\n",
        "        self.fc = nn.Linear(128 * len(window_sizes), 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply a convolution + max pool layer for each window size\n",
        "        x = torch.unsqueeze(x, 1)  # [B, C, T, E] Add a channel dim.\n",
        "        xs = []\n",
        "        for conv in self.convs:\n",
        "            x2 = F.relu(conv(x))  # [B, F, T, 1]\n",
        "            x2 = torch.squeeze(x2, -1)  # [B, F, T]\n",
        "            x2 = F.max_pool1d(x2, x2.size(2))  # [B, F, 1]\n",
        "            xs.append(x2)\n",
        "        x = torch.cat(xs, 2)  # [B, F, window]\n",
        "\n",
        "        # FC\n",
        "        x = x.view(x.size(0), -1)  # [B, F * window]\n",
        "        logits = self.fc(x)  # [B, class]\n",
        "        probs = torch.sigmoid(logits).view(-1)\n",
        "        return probs\n",
        "\n",
        "    def loss(self, probs, targets):\n",
        "        return nn.BCELoss()(probs.float(), targets.float())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeKUM4cb03kc"
      },
      "source": [
        "model = Model()\n",
        "if DEVICE == torch.device('cuda'):\n",
        "    model.cuda()\n",
        "else:\n",
        "    model.cpu()\n",
        "model.train()\n",
        "\n",
        "optimizer = Adam(\n",
        "    [p for p in model.parameters() if p.requires_grad], betas=(0.9, 0.999), eps=1e-5\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvlyIOzT1R3R"
      },
      "source": [
        "import torch as t\n",
        "for i in tqdm(range(100)):\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    input, target = trainset.next_batch(batch_size, device=DEVICE)\n",
        "    out = model(input)\n",
        "    loss = model.loss(out, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}